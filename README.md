# Visi칩n por computador - Pr치ctica IV
## Autores
 - Juan Carlos Rodr칤guez Ram칤rez
 - Mohamed O. Haroun Zarkik

## Introducci칩n
Esta pr치ctica trata del aprendizaje y puesta en uso de los modelos de detecci칩n en una fase (YOLO), y del aprendizaje y uso de los modelos OCR para la detecci칩n de texto.

## Entorno y librer칤as
Para el funcionamiento de esta pr치ctica ser치 necesario tener mucha paciencia para instalar todas las dependencias necesarias en el o los entornos.

```bash
conda create -n VC_P4 python=3.10.19 -y
conda activate VC_P4
conda install pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia -y
conda install -c conda-forge ultralytics opencv pandas easyocr pillow -y
```

## Tarea I
Este proyecto desarrolla un prototipo para el procesamiento de v칤deo que permite:

- Detectar y seguir personas y veh칤culos presentes en el v칤deo.
- Detectar las matr칤culas de los veh칤culos.
- Contar el total de instancias de cada clase.
- Generar un v칤deo anotado visualmente con los resultados de detecci칩n y seguimiento.
- Crear un archivo CSV con el detalle de detecci칩n y seguimiento, con los campos:
  `fotograma, tipo_objeto, confianza, identificador_tracking, x1, y1, x2, y2, matr칤cula_en_su_caso, confianza_matricula, mx1, my1, mx2, my2, texto_matricula`.

## Entrenamiento de Modelos
Para este proyecto se entrenaron dos modelos YOLOv11:

- **YOLOv11 Nano**: dise침ado para detecciones r치pidas, es un modelo muy ligero y eficiente para dispositivos con recursos limitados.
- **YOLOv11 Small**: un modelo un poco m치s pesado, con una arquitectura y n칰mero de par치metros superiores que permiten mayor precisi칩n.

Ambos modelos fueron entrenados usando el mismo [c칩digo](training/train_slp.py) base y conjuntos de hiperpar치metros. Para optimizar estos 칰ltimos, se utiliz칩 un [c칩digo](training/tunning_slp.py) con la funci칩n model.tune de YOLO, que facilita la b칰squeda de los hiperpar치metros 칩ptimos seg칰n el dataset empleado. Se limitaron las iteraciones a 20 para mantener una b칰squeda eficaz pero no excesivamente exhaustiva.

## Dataset
El dataset fue construido combinando im치genes propias junto con im치genes tomadas de varios datasets, entre ellos uno de [Roboflow](https://universe.roboflow.com/licenseplates-h9qfr/spanish-license-plates). Actualmente, el [dataset propio](https://www.kaggle.com/datasets/juanrodrguez215/spanish-plates) est치 disponible en Kaggle de manera p칰blica. Ambos modelos fueron entrenados con el mismo dataset sin ninguna variaci칩n.

Como se comentaba, algunas de las im치genes del dataset fueron de cosecha propia. Por ende, tendr칤an que anotarse, y para ello se hizo uso de la herramienta de etiquetado [CVAT](https://www.cvat.ai/). La herramienta permite exportar las anotaciones en diferentes formatos, lo cual es una gran ventaja. Para las matr칤culas, se hizo la exportaci칩n en formato [YOLO](https://docs.ultralytics.com/es/datasets/detect/#usage-example_1). Para las matr칤culas con su contenido, en formato [ICDAR Recognition](https://docs.cvat.ai/docs/manual/advanced/formats/format-icdar/).

## An치lisis del c칩digo
Es necesario comentar algunos aspectos del c칩digo empleado para la detecci칩n de instancias en el v칤deo:
1. Para la detecci칩n de coches y personas, es necesario usar un modelo que sea capaz de ello, y por ende se utilizaron tanto el YOLOv11 Nano como Small. Para la detecci칩n de matr칤culas, se usaba el entrenado propiamente.
```python
model_objects = YOLO("yolo11n.pt")       # Modelo para personas y veh칤culos
model_plates = YOLO("yolo11n_best.pt")   # Modelo matr칤cula
```

2. Existe un problema con la detecci칩n y conteo en un v칤deo, y es que muy complicado no volver a contar una instancia cada vez que se detecta, a pesar de ser la misma. Cuando un coche estacionado es detectado, se cuenta. Sin embargo, si otro objeto se interpone entre el coche y la c치mara, tras volver a mantener contacto visual, el coche es contado de nuevo. Es por ello que se ha intentado implementar una t칠cnica de IoU, para evitar contear varias veces instancias inm칩viles, sin resultados muy notables.
```python
def rectangles_iou(boxA, boxB):
    # Calcula el IoU entre 2 cajas: boxA y boxB = (x1,y1,x2,y2)
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    iou = interArea / float(boxAArea + boxBArea - interArea) if (boxAArea + boxBArea - interArea) > 0 else 0
    return iou

def reidentify_track(new_box, new_plate_info, frame_num, max_iou=0.5, max_frame_gap=30):
    # Intentar encontrar un track_id anterior para la nueva detecci칩n usando matr칤cula y posici칩n.
    for tid, (cached_plate, last_frame, cached_box) in plate_cache.items():
        if frame_num - last_frame > max_frame_gap:
            continue  # Muy viejo, descartar
        
        # Comparar matr칤cula si existe
        if new_plate_info and cached_plate:
            if new_plate_info['text'] == cached_plate['text']:
                # Matricula coincide: es el mismo objeto (veh칤culo)
                return tid
        
        # Sin matr칤cula o no coincide, comparar bounding boxes (IoU)
        iou = rectangles_iou(new_box, cached_box)
        if iou > max_iou:
            return tid  # Es el mismo objeto con movimiento razonable
    
    return None  # No encontrado
```

3. Para la detecci칩n de matr칤culas, se ha forzado al modelo a detectarlas cada 5 segundos, otorgando esa fluidez y calidad a la detecci칩n. Con 10 o m치s frames de espera, la detecci칩n no es tan buena.
```python
if cls in VEHICLE_CLASSES:
    if frame_idx % 5 == 0:
       plate_info = detect_plate_in_vehicle_frame(frame, box, frame_idx)
```

## Resultados
https://github.com/user-attachments/assets/a1adb621-ea52-451d-9fcd-e1c7b9bfc8cc

### Detecciones realizadas - Conteo de clases

| Clase        | YOLOv11n | YOLOv11s |
|--------------|----------|----------|
| bus          | 5        | 2        |
| car          | 180      | 182      |
| person       | 33       | 39       |
| truck        | 7        | 24       |
| motorcycle   | 5        | 4        |
| Matr칤culas   | 187      | 197      |

En conclusi칩n, el modelo Nano parece ser ligeramente mejor, dudando menos en etiquetar una instancia y haciendo un buen seguimiento a diferencia del modelo Small. En ocasiones, el modelo Small parecee estar detectando matr칤culas fantasmas, o de alguna manera residuales de coches que ya pasaron. No obstante, se nota alguna mejor칤a con respecto al Nano, pues con instancias a lejanas distancias hace un mejor tracking y no duda tanto. De manera general, parece que el IoU ayuda, aunque no demasiado, a no perder la pista de los coches y redectarlos con su ID inicial, siendo este el mayor problema de la pr치ctica. En c칩mputo total, parece que el Nano ha realizado un mejor trabajo, y sorprendentemente, parece ser m치s fiable. En la tarea II, veremos m치s notablemente la importancia de detectar correctamente las matr칤culas para su posterior lectura.

---

## Tarea II

Mientras que la Tarea I se centr칩 en la detecci칩n y seguimiento de objetos (veh칤culos, personas y matr칤culas) usando YOLOv11, esta tarea aborda un desaf칤o m치s espec칤fico: el **Reconocimiento 칍ptico de Caracteres (OCR)**.

Como se observ칩 en los resultados de la Tarea I, los modelos YOLO son excelentes para *localizar* la matr칤cula, pero no para *leer* el texto que contiene. La Tarea II se enfoca en implementar, entrenar y comparar modelos dise침ados espec칤ficamente para leer el texto de las matr칤culas detectadas.

Para ello, se utilizan dos *notebooks*:
1.  **`entrenamiento-ocr.ipynb`**: Entrena un modelo OCR personalizado (una CRNN) desde cero.
2.  **`VC_P4_B.ipynb`**: Compara el modelo personalizado contra una librer칤a popular (EasyOCR) en un v칤deo de prueba.

---

### 1. Entrenamiento del Modelo OCR (`entrenamiento-ocr.ipynb`)

Este *notebook* detalla el proceso completo de creaci칩n de un modelo OCR propio, especializado en la lectura de matr칤culas espa침olas.

#### Librer칤as Empleadas
* **torch / torchvision**: El framework principal para construir y entrenar la red neuronal.
    * `pip install torch torchvision`
* **pandas**: Utilizado para cargar y gestionar las etiquetas (el texto de cada matr칤cula) desde el archivo `gt.txt`.
    * `pip install pandas`
* **opencv-python (cv2)**: Necesario para cargar las im치genes de las matr칤culas y aplicar pre-procesamiento (cambio de tama침o, padding).
    * `pip install opencv-python`
* **numpy**: Para operaciones num칠ricas y manipulaci칩n de im치genes.
    * `pip install numpy`

#### Paso 1: Definici칩n del Modelo (CRNN)

Se implementa una arquitectura **CRNN (Convolutional Recurrent Neural Network)**, un est치ndar de la industria para el reconocimiento de texto.

* **Parte Convolucional (CNN)**: Una serie de capas `Conv2d` y `MaxPool2d` act칰an como un extractor de caracter칤sticas. Aprenden a identificar patrones visuales (l칤neas, curvas, formas) en la imagen de la matr칤cula.
* **Parte Recurrente (RNN)**: La salida de la CNN se "aplana" y se pasa a una `LSTM` (Long Short-Term Memory). Esta red recurrente procesa la secuencia de caracter칤sticas de izquierda a derecha, aprendiendo el orden y la relaci칩n entre los caracteres.
* **Capa Final**: Una capa `Linear` proyecta la salida de la LSTM al n칰mero de clases (los 37 caracteres posibles: 0-9, A-Z y el car치cter *'blank'*).

```python
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 64, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            # ... m치s capas convolucionales ...
            nn.Conv2d(256, 512, 3, 1, 1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d((2,1), (2,1))
        )
        
        # 游댳 Cambiamos el input_size para que coincida con la salida real del CNN
        self.rnn = nn.LSTM(1024, 256, num_layers=2, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.cnn(x)
        b, c, h, w = x.size()
        x = x.view(b, c*h, w).permute(0, 2, 1)  # (batch, width, features)
        x, _ = self.rnn(x)
        x = self.fc(x)
        return x
```

#### Paso 2: Carga y Procesamiento de Datos

Se crea una clase `PlatesDataset` personalizada que se encarga de:
1.  Leer el `gt.txt` y asociar cada imagen con su texto.
2.  Cargar cada imagen en escala de grises.
3.  **Redimensionar y rellenar (padding)**: Todas las im치genes se fuerzan a un tama침o fijo (ej. 128x32 p칤xeles) para que puedan procesarse en lotes (batches), manteniendo la relaci칩n de aspecto.
4.  **Codificar el texto**: Convierte el texto (ej. "4517MFC") en una secuencia de 칤ndices num칠ricos (ej. `[14, 15, 11, 17, 22, 15, 12]`) que la red pueda entender.

```python
class PlatesDataset(Dataset):
    def __init__(self, df, img_dir, transform=None, max_len=10):
        # ... (inicializaci칩n) ...
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(self.img_dir, row['filename'])
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        
        # ... (manejo de im치genes corruptas) ...

        # resize manteniendo ratio y rellenando
        h, w = img.shape
        new_h = IMG_H
        new_w = int(w * (IMG_H / h))
        new_w = min(new_w, IMG_W)
        img = cv2.resize(img, (new_w, new_h))
        if new_w < IMG_W:
            pad = np.full((IMG_H, IMG_W - new_w), 255, dtype=np.uint8)
            img = np.concatenate([img, pad], axis=1)
        
        # ... (conversi칩n a tensor y etiquetas) ...
        
        labels = self.text_to_labels(row['text'])
        return torch.tensor(img).float(), torch.tensor(labels).int(), len(labels)
```

### Paso 3: Entrenamiento con CTCLoss

El modelo se entrena usando nn.CTCLoss (Connectionist Temporal Classification). Esta funci칩n de p칠rdida es fundamental para el OCR: permite al modelo aprender a predecir la secuencia de caracteres correcta sin necesidad de saber la ubicaci칩n exacta de cada letra en la imagen. Simplemente se le da la imagen y el texto final, y la CTCLoss se encarga de alinear la predicci칩n de la red con la etiqueta real.

El dataset se divide (90% entrenamiento, 10% validaci칩n) y se entrena durante 30 칠pocas, guardando el modelo con la menor p칠rdida de validaci칩n.

2. Comparativa de Modelos OCR (VC_P4_B.ipynb)

Este notebook toma el modelo entrenado (ocr_v3.pt) y lo compara en un escenario real contra la popular librer칤a EasyOCR.

  Librer칤as Empleadas

    - ultralytics: Para cargar el modelo YOLOv11 (de la Tarea I) y detectar las matr칤culas en el v칤deo.

      - pip install ultralytics

    - easyocr: La librer칤a de OCR pre-entrenada que usaremos como baseline para la comparaci칩n.

      - pip install easyocr

    - torch: Para cargar y ejecutar nuestro modelo CRNN personalizado.

      - pip install torch

    - pandas: Para almacenar los resultados de la comparaci칩n en un archivo CSV.

      - pip install pandas

    - opencv-python (cv2): Para leer el v칤deo de entrada (plates_test.mp4) fotograma a fotograma.

      - pip install opencv-python

#### Proceso de Comparaci칩n

1. **Carga de Modelos**: Se cargan tres componentes:

      - El detector YOLO (yolo11n_best.pt).

      - El lector de EasyOCR (easyocr.Reader(['es', 'en'])).

      - Nuestro CRNN personalizado (ocr_v3.pt), junto con su definici칩n de clase y transformaciones de imagen (escala de grises, redimensionado a 32x128).

2. **Procesamiento del V칤deo**: El script itera sobre cada fotograma del v칤deo plates_test.mp4.

      - **Detecci칩n (YOLO)**: Primero, YOLO detecta la posici칩n (x1, y1, x2, y2) de cualquier matr칤cula en el fotograma.

      - **Recorte (Crop)**: La regi칩n de la matr칤cula se recorta de la imagen original.

      - **Inferencia (OCR)**: Esta imagen recortada se env칤a a ambos modelos de OCR:

          - EasyOCR procesa la imagen directamente.

          - La imagen se pre-procesa (transforma) y se env칤a al modelo CRNN.

      - **Almacenamiento**: El texto predicho por ambos modelos se guarda en una lista.

3.  Generaci칩n de Resultados: Al finalizar el v칤deo, todos los resultados se vuelcan a un archivo CSV (comparacion_ocr_v3_yolo11n.csv). Este archivo permite un an치lisis detallado, fotograma a fotograma, de qu칠 modelo fue m치s preciso, cu치ntos fallos tuvo cada uno y en qu칠 fotogramas espec칤ficos se produjeron los errores.

A continuaci칩n, se comentan las partes clave del script de comparaci칩n (`VC_P4_B.ipynb`).

#### Bloque 1: Carga de Detectores y Lectores

Antes de procesar el v칤deo, se inicializan los modelos principales.

En primera instancia, se realizan las siguientes operaciones:
- Se define el device (CPU o CUDA) para la ejecuci칩n.

- Se carga el detector YOLOv11 entrenado para localizar las matr칤culas.

- Se inicializa el lector de EasyOCR, que ser치 nuestro modelo base de comparaci칩n.
```python
device = "cuda" if torch.cuda.is_available() else "cpu"

# Detector YOLO (de la Tarea I)
detector = YOLO("models/yolo11n_best.pt")

# EasyOCR (idiomas espa침ol e ingl칠s)
reader_easy = easyocr.Reader(['es', 'en'])
```

Aqu칤 tienes el desglose final en formato Markdown:
Markdown

---

### 3. Desglose del C칩digo de Comparaci칩n (`VC_P4_B.ipynb`)

A continuaci칩n, se comentan las partes clave del script de comparaci칩n (`VC_P4_B.ipynb`).

#### Bloque 1: Carga de Detectores y Lectores

Antes de procesar el v칤deo, se inicializan los modelos principales.

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

# Detector YOLO (de la Tarea I)
detector = YOLO("models/yolo11n_best.pt")

# EasyOCR (idiomas espa침ol e ingl칠s)
reader_easy = easyocr.Reader(['es', 'en'])

    Se define el device (CPU o CUDA) para la ejecuci칩n.

    Se carga el detector YOLOv11 entrenado para localizar las matr칤culas.

    Se inicializa el lector de EasyOCR, que ser치 nuestro modelo base de comparaci칩n.
```

#### Bloque 2: Carga del Modelo CRNN Personalizado

Para cargar un modelo PyTorch (.pt), es necesario tener definida su arquitectura (la clase CRNN) en el script. Para lo cual:
- Se define la clase CRNN exactamente igual que en el notebook de entrenamiento.

- Se carga el archivo .pt con los pesos entrenados y se env칤a al device.

- model_crnn.eval() es crucial para desactivar capas como BatchNorm o Dropout, asegurando que la inferencia sea consistente. 

Brevemente, se realizan las siguientes tareas en el siguiente fragmento de c칩digo:

- transform: Define la canalizaci칩n de pre-procesamiento: escala de grises, redimensi칩n a 32x128 p칤xeles y conversi칩n a Tensor.

- CHARS: El mapa de caracteres que el modelo puede predecir.

- decode_ctc: Funci칩n clave que toma la salida de la red (una matriz de probabilidades) y la colapsa en texto legible, eliminando duplicados y caracteres "blank" (vac칤os).

```python
# Transformaciones para CRNN
transform = T.Compose([
    T.Grayscale(),
    T.Resize((32, 128)), # Tama침o fijo (H, W) con el que se entren칩
    T.ToTensor(),
])

# Diccionario de caracteres (debe ser id칠ntico al de entrenamiento)
CHARS = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
idx_to_char = {i: c for i, c in enumerate(CHARS)}

def decode_ctc(output):
    """Decodifica la salida CTC en texto."""
    pred = output.softmax(2).argmax(2).squeeze(0).cpu().numpy()
    text = ""
    prev_char = -1
    for c in pred:
        if c != prev_char and c < len(CHARS):
            text += idx_to_char.get(c, "")
        prev_char = c
    return text
```

#### Bloque 3: Transformaciones y Decodificador CTC

El modelo CRNN no acepta una imagen en crudo. Requiere transformaciones espec칤ficas y una funci칩n para decodificar su salida.

#### Bloque 4: Bucle Principal de Procesamiento de V칤deo

Esta es la secci칩n central que itera sobre el v칤deo, detecta y compara los OCR. Se puede dividir en las siguientes partes:

---

##### 4.1: Inicializaci칩n y Lectura del V칤deo

Primero, abrimos el archivo de v칤deo y preparamos las variables para el bucle.

```python
VIDEO = "plates_test.mp4"
cap = cv2.VideoCapture(VIDEO)
fps = cap.get(cv2.CAP_PROP_FPS)
frame_count = 0
data_rows = []

print("Procesando v칤deo...")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_count += 1
    timestamp = datetime.fromtimestamp((frame_count / fps)).strftime("%H:%M:%S.%f")[:-3]
```

- cv2.VideoCapture(VIDEO): Abre el archivo de v칤deo (plates_test.mp4).

- cap.get(cv2.CAP_PROP_FPS): Obtiene la tasa de fotogramas por segundo (FPS) del v칤deo. Esto es vital para calcular el timestamp (marca de tiempo).

- data_rows = []: Inicializa la lista que almacenar치 todos nuestros resultados antes de guardarlos en un CSV.

- while cap.isOpened(): Inicia el bucle que se ejecutar치 mientras el v칤deo est칠 abierto.

- ret, frame = cap.read(): Lee un 칰nico fotograma. ret es un booleano que indica si la lectura fue exitosa, y frame es la imagen en s칤 (como un array de NumPy).

- if not ret: break: Si ret es False, significa que el v칤deo ha terminado, por lo que salimos del bucle.

- timestamp = ...: Calcula la marca de tiempo exacta del fotograma actual dividiendo el n칰mero de fotograma (frame_count) por los FPS.

##### 4.2: Detecci칩n y Recorte de la Matr칤cula

Dentro del bucle, por cada fotograma, primero usamos YOLO (de la Tarea I) para encontrar la matr칤cula y luego la recortamos.

```python
    # 1. Detecci칩n con YOLO
    results = detector(frame, verbose=False)

    if results[0].boxes:
        for box in results[0].boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            placa = frame[y1:y2, x1:x2] # 2. Recorte de la matr칤cula
            if placa.size == 0:
                continue
```
- results = detector(frame, verbose=False): Pasa el fotograma completo al modelo YOLO (detector) para que encuentre objetos. verbose=False evita que imprima informaci칩n de detecci칩n en la consola por cada fotograma.

- if results[0].boxes:: Comprueba si YOLO realmente detect칩 alguna caja (matr칤cula) en este fotograma.

- for box in results[0].boxes:: Itera sobre todas las matr칤culas encontradas (en caso de que haya m치s de una).

- x1, y1, x2, y2 = map(int, ...): Extrae las coordenadas de la caja detectora.

- placa = frame[y1:y2, x1:x2]: Este es el recorte. Usando slicing de NumPy, seleccionamos solo la regi칩n de inter칠s (la matr칤cula) del fotograma original. Esta imagen placa es la que se usar치 para el OCR.

- if placa.size == 0:: Una comprobaci칩n de seguridad. Si el recorte falla y produce una imagen vac칤a, saltamos esta detecci칩n y continuamos con la siguiente.

##### 4.3: Inferencia con EasyOCR

Enviamos la imagen recortada (placa) al primer modelo: EasyOCR.

```python
# 3. Inferencia con EASY OCR
            try:
                text_easy = reader_easy.readtext(placa, detail=0, allowlist=CHARS)
                text_easy = max(text_easy, key=len).replace(" ", "") if text_easy else ""
            except:
                text_easy = ""
```

- try...except...: Se usa un bloque try porque el proceso de OCR puede fallar (por ejemplo, si la imagen es puro ruido). Si falla, simplemente asignamos un texto vac칤o "".

- reader_easy.readtext(placa, ...): Ejecuta la inferencia de EasyOCR sobre la imagen recortada.

- detail=0: Indica a EasyOCR que devuelva solo una lista de strings con el texto, en lugar de objetos con coordenadas y confianza.

- allowlist=CHARS: Una optimizaci칩n clave. Restringe a EasyOCR para que solo reconozca los caracteres que le pasamos (nuestro alfabeto 0-9 y A-Z), ignorando s칤mbolos o letras raras.

- max(text_easy, key=len)...: A veces, readtext puede devolver varios fragmentos (ej. ['4517', 'MFC']). Este c칩digo toma el fragmento m치s largo (o el 칰nico, si solo hay uno) y elimina los espacios.

##### 4.4: Inferencia con CRNN (Modelo Propio)

A continuaci칩n, enviamos la misma imagen recortada a nuestro modelo CRNN personalizado.

```python
# 4. Inferencia con CRNN (modelo propio)
            try:
                placa_rgb = cv2.cvtColor(placa, cv2.COLOR_BGR2RGB)
                img_pil = Image.fromarray(placa_rgb)
                img_t = transform(img_pil).unsqueeze(0).to(device)
                with torch.no_grad():
                    out = model_crnn(img_t)
                text_crnn = decode_ctc(out)
            except:
                text_crnn = ""
```

- placa_rgb = cv2.cvtColor(placa, cv2.COLOR_BGR2RGB): OpenCV carga im치genes en formato BGR (Azul, Verde, Rojo). Las transformaciones de PyTorch (transform) esperan formato RGB. Esta l칤nea corrige el orden de los canales de color.

- img_pil = Image.fromarray(placa_rgb): Convierte la imagen de un array de NumPy (formato OpenCV) a un objeto de imagen PIL (formato que esperan las transformaciones).

- img_t = transform(img_pil): Aplica la secuencia de transformaciones definida en el Bloque 3 (escala de grises, redimensionado a 32x128, conversi칩n a Tensor).

- .unsqueeze(0): Nuestro modelo espera un "lote" (batch) de im치genes. Esta funci칩n a침ade una dimensi칩n extra al principio, convirtiendo la forma de [Canales, Alto, Ancho] a [1, Canales, Alto, Ancho], simulando un lote de tama침o 1.

- with torch.no_grad(): Desactiva el c치lculo de gradientes. Es una optimizaci칩n crucial durante la inferencia, ya que reduce el uso de memoria y acelera el proceso (no estamos entrenando).

- out = model_crnn(img_t): Ejecuta la inferencia de nuestro modelo CRNN.

- text_crnn = decode_ctc(out): Usa la funci칩n auxiliar (definida en el Bloque 3) para convertir la salida cruda del modelo (probabilidades) en un string de texto limpio.

##### 4.5: Almacenamiento de Resultados

Finalmente, agrupamos los resultados de ambas inferencias y los a침adimos a nuestra lista. Se crea un diccionario que contiene el n칰mero de fotograma, la marca de tiempo y el texto predicho por ambos modelos para esta detecci칩n espec칤fica. Al final del v칤deo, esta lista contendr치 el historial completo de todas las detecciones.

```python
# 5. Almacenamiento de resultados
            data_rows.append({
                "Frame": frame_count,
                "Tiempo": timestamp,
                "EasyOCR": text_easy,
                "CRNN_Custom": text_crnn
            })
```

#### Bloque 5: Guardado de Resultados

Al finalizar el bucle, se liberan los recursos y se guardan los datos recopilados en un archivo CSV usando pandas.

```python
cap.release()

# Guardar resultados
df = pd.DataFrame(data_rows)
df.to_csv("comparacion_ocr_v3_yolo11n.csv", index=False)
print("Comparaci칩n completada.")
```

## An치lisis y comparativa de resultados
Este es un breve an치lisis comparativo del **rendimiento de detecci칩n** (la tasa de lecturas no nulas) de los diferentes m칠todos de OCR (EasyOCR, CRNN_Custom, Tesseract) basado en los tres archivos CSV proporcionados.

### Tasa de Lecturas (No Nulas vs. Placeholder '0')

La siguiente tabla resume cu치ntas lecturas v치lidas (definidas como una salida no nula o, en el caso del archivo antiguo, una salida que no sea `0`) produjo cada m칠todo.

| Archivo / Modelo | Total de Filas | Lecturas EasyOCR | Lecturas CRNN_Custom | Lecturas Tesseract |
| :--- | :---: | :---: | :---: | :---: |
| `...yolo11n.csv` (YOLOv11 Nano) | 172 | 98 (57.0%) | 172 (100.0%) | N/A |
| `...yolo11s.csv` (YOLOv11 Small) | 254 | 28 (11.0%) | 254 (100.0%) | N/A |
| `...tessaract.csv` (Antigua) | 204 | 106 (52.0%) | 0 (0.0%) | 0 (0.0%) |

---

### Conclusiones Clave

1.  **Rendimiento de `CRNN_Custom`:** Este modelo muestra dos comportamientos completamente diferentes:
    * En las pruebas con **YOLO (`...yolo11n.csv` y `...yolo11s.csv`)**, tiene una tasa de respuesta del 100%. Esto significa que *siempre* devuelve un valor.
    * En la prueba **`...tessaract.csv` (Antigua)**, el modelo usaba `0` como valor "placeholder" (marcador de posici칩n) para indicar "no lectura", resultando en 0 lecturas v치lidas.

2.  **Rendimiento de `EasyOCR`:** El rendimiento de `EasyOCR` parece depender en gran medida del detector de matr칤culas utilizado.
    * Tuvo su peor rendimiento con el detector `YOLOv11s` (solo un 11.0% de lecturas).
    * Tuvo un rendimiento moderado con `YOLOv11n` (57.0%) y con el detector del archivo "Antiguo" (52.0%).

3.  **Rendimiento de `Tesseract`:** En el conjunto de datos "Antiguo" donde fue probado, `Tesseract` no produjo **ninguna** lectura v치lida (0%).

4.  **Tasa de Respuesta vs. Precisi칩n:** Es importante notar que una "tasa de lectura" del 100% (como la de `CRNN_Custom` en los archivos YOLO) no implica un 100% de *precisi칩n*. Simplemente significa que el modelo siempre genera una salida. Por el contrario, `EasyOCR` parece devolver un valor solo cuando detecta una matr칤cula con un nivel de confianza suficiente.